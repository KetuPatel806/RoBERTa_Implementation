{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1772511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jhanvi/Desktop/DeepLearning/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizer, get_cosine_schedule_with_warmup\n",
    "from torch import optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "066b7208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special Tokens in Tokenizer\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}\n",
      "Special_token_dict\n",
      "{'<s>': 0, '</s>': 2, '<unk>': 3, '<pad>': 1, '<mask>': 50264}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "print(\"Special Tokens in Tokenizer\")\n",
    "special_tokens = tokenizer.special_tokens_map\n",
    "print(special_tokens)\n",
    "\n",
    "special_tokens_dict = {}\n",
    "\n",
    "for token in special_tokens.values():\n",
    "    special_tokens_dict[token] = tokenizer.encode(token)[1]\n",
    "print(\"Special_token_dict\")\n",
    "print(special_tokens_dict)\n",
    "\n",
    "\n",
    "# Let see here we need the 1th element for the storing the embedding values for the tokens\n",
    "# {'<s>': [0, 0, 2],\n",
    "#  '</s>': [0, 2, 2],\n",
    "#  '<unk>': [0, 3, 2],\n",
    "#  '<pad>': [0, 1, 2],\n",
    "#  '<mask>': [0, 50264, 2]}\n",
    "\n",
    "all_tokens_idx = list(range(tokenizer.vocab_size))\n",
    "## print(f\"all_tokens_idx: {all_tokens_idx}\")\n",
    "all_special_tokens_idx = sorted(list(special_tokens_dict.values()))\n",
    "## print(f\"all_special_tokens_idx: {all_special_tokens_idx}\")\n",
    "all_non_special_tokens_idx = [token for token in all_tokens_idx if token not in all_special_tokens_idx]\n",
    "## print(f\"all_non_special_tokens_idx: {all_non_special_tokens_idx}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbd1e2b",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4996d7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "path_to_data = \"/Users/jhanvi/Desktop/DeepLearning/RoBERTa_Implementation/HarryPotter\"\n",
    "\n",
    "text_files = os.listdir(path_to_data)\n",
    "all_text = \"\"\n",
    "\n",
    "for book in text_files:\n",
    "    with open(os.path.join(path_to_data, book), \"r\") as f:\n",
    "        text = f.readlines() # Read in all lines\n",
    "        text = [line for line in text if \"Page\" not in line] # Remove lines with Page Numbers\n",
    "        text = \" \".join(text).replace(\"\\n\", \"\") # Remove all newline characters\n",
    "        text = [word for word in text.split(\" \") if len(word) > 0] # Remove all empty characters\n",
    "        text = \" \".join(text) # Combined lightly cleaned text\n",
    "        all_text += text\n",
    "\n",
    "### Split Data by \"sentences\" ###\n",
    "all_text = all_text.split(\".\")\n",
    "\n",
    "### Grab 5 Sentences at a time and put them together by the period ###\n",
    "all_text_chunked = [\".\".join(all_text[i:i+5]) for i in range(0, len(all_text), 5)]\n",
    "\n",
    "### Tokenize all the text! ###\n",
    "tokenized_text = [tokenizer.encode(text) for text in all_text_chunked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5815e2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2121, 50264, 50264, 12729, 50264, 16877, 50264, 50264, 50264, 50264,\n",
      "        50264, 50264])\n",
      "tensor([ 1589,  1941,    13,    23,   427,  1368, 23894,    31,    39,  3268,\n",
      "           17,    27])\n",
      "tensor([    0,   863,   479,   229,   479,   248,   384,   305,   226, 27785,\n",
      "          234,   272, 32721, 16802,   221,  3293,  7831,  2121, 50264, 31534,\n",
      "         4014,   163,  5216,  3732,  2774,  1491, 50264,     5,    78,    86,\n",
      "            6,    41,  4795,    56,  3187,    66,    81,  7080, 12729,   346,\n",
      "          237,     6,  9522, 18068,  3936,     4, 50264,     4, 14571,   211,\n",
      "         4668,   607,    56,    57,   885, 22036,    11,     5,   419,   722,\n",
      "            9,     5,   662,    30,    10,  7337,     6, 16877, 50264,  6496,\n",
      "        50264, 50264, 11884, 50264, 50264, 50264,    29,   929,     2])\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  1589,  1941,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,    13,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,    23,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,   427,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  1368, 23894,  -100,\n",
      "           31,    39,  -100,  3268,    17,    27,  -100,  -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "class MaskedLMLoader(Dataset):\n",
    "    def __init__(self, tokenized_data, max_seq_len=100, masking_ratio=0.15):\n",
    "        self.data = tokenized_data\n",
    "        self.mask_ratio = masking_ratio\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _random_mask_text(self, tokens):\n",
    "\n",
    "        ### Create Random Uniform Sample Tensor ###\n",
    "        random_masking = torch.rand(*tokens.shape)## This are random numbers from range 0 to 1 with the tokens shape\n",
    "\n",
    "        ### Set Value of Special Tokens to 1 so we DONT MASK THEM ###\n",
    "        special_tokens = torch.tensor(tokenizer.get_special_tokens_mask(tokens, already_has_special_tokens=True)) ## here we create the special token tensor where the data is 0/1\n",
    "        random_masking[special_tokens==1] = 1\n",
    "\n",
    "        ### Get Boolean of Words under Masking Threshold ###\n",
    "        random_masking = (random_masking < self.mask_ratio)\n",
    "\n",
    "        ### Create Labels ###\n",
    "        labels = torch.full((tokens.shape), -100) ## this is full metrix/tensor with value of -100 with shape of tokens\n",
    "        labels[random_masking] = tokens[random_masking] ## Create the labels with -100 and the masking tokens\n",
    "\n",
    "        ### Get Indexes of True ###\n",
    "        random_selected_idx = random_masking.nonzero() ## Select the index with non-zero values\n",
    "\n",
    "        ### 80% Of the Time Replace with Mask Token ###\n",
    "        masking_flag = torch.rand(*random_selected_idx.shape) ## Masking flag create the matrix with the random selected index with the shape\n",
    "        masking_flag = (masking_flag<0.8) ## Masking flag < 80% like 0.56 < 0.80\n",
    "        selected_idx_for_masking = random_selected_idx[masking_flag] ## Selected data will geting mask\n",
    "\n",
    "        ### Seperate out remaining indexes to be assigned ###\n",
    "        unselected_idx_for_masking = random_selected_idx[~masking_flag] ## Now unselected index for masking like 20% data\n",
    "\n",
    "        ### 10% of the time (or 50 percent of the remaining 20%) we fill with random token ###\n",
    "        ### The remaining times, leave the text as is ###\n",
    "        masking_flag = torch.rand(*unselected_idx_for_masking.shape) ## Now that 10% from the 20% are going to be the masking \n",
    "        masking_flag = (masking_flag<0.5)\n",
    "        selected_idx_for_random_filling = unselected_idx_for_masking[masking_flag] \n",
    "        selected_idx_to_be_left_alone = unselected_idx_for_masking[~masking_flag] ## this index is are not changed that are left alone \n",
    "        \n",
    "        ### Fill Mask Tokens ###\n",
    "        if len(selected_idx_for_masking) > 0:\n",
    "            tokens[selected_idx_for_masking] = special_tokens_dict[\"<mask>\"]\n",
    "        \n",
    "        ### Fill Random Tokens ###\n",
    "        if len(selected_idx_for_random_filling) > 0:\n",
    "            randomly_selected_tokens = torch.tensor(random.sample(all_non_special_tokens_idx, len(selected_idx_for_random_filling))) ## random sample will be fill in that 10%\n",
    "            tokens[selected_idx_for_random_filling] = randomly_selected_tokens\n",
    "        \n",
    "        \n",
    "        return tokens, labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.tensor(self.data[idx])\n",
    "\n",
    "        ### Make sure sequence is within context length ###\n",
    "        if len(data) > self.max_seq_len:\n",
    "            rand_start_idx = random.choice(list(range(len(data) - self.max_seq_len)))\n",
    "            end_idx = rand_start_idx + self.max_seq_len\n",
    "            data = data[rand_start_idx:end_idx]\n",
    "  \n",
    "        ### Uniform Random Masking ###\n",
    "        masked_tokens, label = self._random_mask_text(data)\n",
    "\n",
    "        return masked_tokens, label\n",
    "\n",
    "mlm = MaskedLMLoader(tokenized_text)\n",
    "\n",
    "for masked_tokens, labels in mlm:\n",
    "    print(masked_tokens[labels!=-100])\n",
    "    print(labels[labels!=-100])\n",
    "    print(masked_tokens)\n",
    "    print(labels)\n",
    "    break\n",
    "\n",
    "# for example let we have the \n",
    "# selected_idx_for_masking = tensor([4,52,35,25,63,633,156,246,1356,90]) ## this data is used for the masking [80% data]\n",
    "# select_index_for_left_alone = tensor([52]) ## which is untouched index which is same as that no masking and no random filling [10% data]\n",
    "# select_index_for_random_filling = tensor([93]) ## which is randomly filled not the accurate filling [10% data]\n",
    "\n",
    "# Here Starting Of sentence is special token represents 0 as well as 2 represents the end of sentence\n",
    "# tensor([    0,   863,   479, 50264,   479,   248,   384,   305,   226, 27785,\n",
    "#           234,   272, 32721, 16802,   221,  3293,  7831,  1589,  1941, 31534,\n",
    "#          4014,   163,  5216,  3732, 50264,  1491, 50264,     5,    78, 50264,\n",
    "#         40588,    41,  4795,    56,  3187,    66,    81,  7080, 14673,   346,\n",
    "#           237,     6,  9522, 18068,  3936,     4,   427,     4, 14571,   211,\n",
    "#          4668,   607,    56, 50264,   885, 22036,    11,     5,   419,   722,\n",
    "#             9,     5,   662,    30,    10,  7337,     6, 50264, 23894,  6496,\n",
    "#            31,    39, 11884,  3268,    17,    27, 50264,   929,     2])\n",
    "# here we see that masking with -100 but the specific data is not masked so that data is used in the furthur implementation for true prediction\n",
    "# tensor([-100, -100, -100,  229, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "#         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "#         2774, -100,   13, -100, -100,   86,    6, -100, -100, -100, -100, -100,\n",
    "#         -100, -100,   23, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "#         -100, -100, -100, -100, -100,   57, -100, -100, -100, -100, -100, -100,\n",
    "#         -100, -100, -100, -100, -100, -100, -100, 1368, -100, -100, -100, -100,\n",
    "#         -100, -100, -100, -100,   29, -100, -100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3fee43",
   "metadata": {},
   "source": [
    "### Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a73177b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    \n",
    "    token_samples = []\n",
    "    label_samples = []\n",
    "    \n",
    "    for tokens, labels in batch:\n",
    "        \n",
    "        token_samples.append(tokens)\n",
    "        label_samples.append(labels)\n",
    "        \n",
    "    max_len = max([len(token) for token in token_samples]) ## Here the max_len in the whole dataset is 100\n",
    "    print(max_len)\n",
    "    \n",
    "    padding_masks = []\n",
    "    \n",
    "    for idx in range(len(token_samples)):\n",
    "        sample = token_samples[idx]\n",
    "        label = label_samples[idx]\n",
    "        \n",
    "        seq_len = len(sample)\n",
    "        \n",
    "        diff = max_len - seq_len ## Difference used when we calculate the max_len to seq_len for padding purpose\n",
    "    \n",
    "        #print(diff)\n",
    "        \n",
    "        if diff > 0:\n",
    "            \n",
    "            padding = torch.tensor([special_tokens_dict['<pad>'] for _ in range(diff)])\n",
    "            ##print(padding)\n",
    "            sample = torch.concatenate((sample, padding))\n",
    "            ##print(sample)\n",
    "            \n",
    "            token_samples[idx] = sample\n",
    "            \n",
    "            ## label padding\n",
    "            label_padding = torch.tensor([-100 for _ in range(diff)])\n",
    "            label = torch.concatenate((label,label_padding))\n",
    "            label_samples[idx] = label\n",
    "            \n",
    "            padding_mask = (sample != special_tokens_dict['<pad>'])\n",
    "            padding_masks.append(padding_mask)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            padding_masks.append(torch.ones(max_len))\n",
    "            \n",
    "    token_samples = torch.stack(token_samples)\n",
    "    label_samples = torch.stack(label_samples)\n",
    "    padding_masks = torch.stack(padding_masks)\n",
    "    \n",
    "    ##print(token_samples.shape, label_samples.shape, padding_masks.shape)\n",
    "    \n",
    "    batch = {\n",
    "        \"input_ids\" : token_samples,\n",
    "        \"labels\" : label_samples,\n",
    "        \"attention_mask\": padding_masks.bool()\n",
    "    }\n",
    "    return batch\n",
    "        \n",
    "loader = DataLoader(mlm, batch_size=64, collate_fn=collate_fn)\n",
    "\n",
    "for sample in loader:\n",
    "    print(sample['attention_mask'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d478e8",
   "metadata": {},
   "source": [
    "### Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f0202ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "        embed_dim=768,\n",
    "        num_heads = 12,\n",
    "        proj_p = 0.0, ## dropout\n",
    "        attn_p = 0.0 ):## Attention Dropout\n",
    "        \n",
    "        super(SelfAttentionEncoder, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(embed_dim,embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim,embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim,embed_dim)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(embed_dim,embed_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "        \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        \n",
    "        q = self.q_proj(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        k = self.k_proj(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        v = self.v_proj(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        attn = q @ k.transpose(-2,-1) * self.head_dim**-0.05\n",
    "        \n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            \n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(1)\n",
    "            attn = attn.masked_fill(~attention_mask, float(\"-inf\"))\n",
    "            \n",
    "        # print(attn)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = attn @ v\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = x.transpose(1,2).flatten(2)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "\n",
    "rand_x = torch.randn(2,5,16)\n",
    "\n",
    "padding = torch.tensor([[True,True,True,True,True],\n",
    "                        [True,True,True,False,False]])\n",
    "\n",
    "attn = SelfAttentionEncoder(embed_dim=16, num_heads=2)\n",
    "output = attn(rand_x,padding)\n",
    "\n",
    "# attention_mask.shape =torch.Size([2, 5])\n",
    "# attn.shape = torch.Size([2, 2, 5, 5]) now we need to convert it to (2,5) -> (2,2,5,5)\n",
    "\n",
    "\n",
    "## Now the output of print(attn) after the \n",
    "# tensor([[[[ 6.0092e-01, -1.1851e+00, -8.4532e-02,  8.2474e-01, -2.0042e-01],\n",
    "#           [ 2.9442e-01, -1.4769e-01,  8.2316e-01,  2.7357e-01, -6.7785e-01],\n",
    "#           [-2.1567e-01,  1.1731e-01, -4.4792e-02, -4.8720e-02, -1.1119e-03],\n",
    "#           [-1.3498e+00,  1.8599e+00, -2.9678e-01, -7.1736e-01,  7.5775e-01],\n",
    "#           [ 1.9210e-01, -1.4102e-01,  2.0816e-01,  3.2017e-01,  1.2134e-01]],\n",
    "\n",
    "#          [[-6.3280e-01,  4.4333e-01, -4.9401e-01,  5.8445e-01,  4.5367e-01],\n",
    "#           [-6.1286e-02, -4.8294e-01,  9.6761e-02, -1.2049e+00, -4.4538e-02],\n",
    "#           [-1.4047e+00,  4.5752e-01, -5.5535e-01, -1.3051e+00,  8.8260e-01],\n",
    "#           [ 6.9645e-01, -1.8630e-01, -3.5097e-01,  7.6276e-01, -2.7011e-01],\n",
    "#           [ 1.5986e+00, -2.6503e-01, -5.9812e-01,  1.5880e+00,  5.5917e-01]]],\n",
    "\n",
    "\n",
    "#         [[[ 8.8883e-01, -6.4672e-01,  1.5378e+00,        -inf,        -inf],\n",
    "#           [ 6.9943e-01, -1.6775e-01, -7.7830e-01,        -inf,        -inf],\n",
    "#           [ 6.8239e-01, -1.6550e+00, -3.5772e-01,        -inf,        -inf],\n",
    "#           [ 1.1485e+00, -5.3262e-01,  1.1484e+00,        -inf,        -inf],\n",
    "#           [-9.4539e-01, -2.0303e-01, -1.9375e+00,        -inf,        -inf]],\n",
    "\n",
    "#          [[-7.7953e-01,  2.3295e-01,  7.3413e-03,        -inf,        -inf],\n",
    "#           [-5.9923e-01,  1.8716e+00,  6.6245e-01,        -inf,        -inf],\n",
    "#           [-4.8800e-02,  6.2469e-01, -4.9597e-01,        -inf,        -inf],\n",
    "#           [-1.4099e+00,  1.6088e+00, -1.7086e-01,        -inf,        -inf],\n",
    "#           [ 6.6090e-01, -1.9622e+00,  6.8945e-01,        -inf,        -inf]]]],\n",
    "#        grad_fn=<MaskedFillBackward0>)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea240be",
   "metadata": {},
   "source": [
    "### Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb2aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_features,\n",
    "                 out_features,\n",
    "                 mlp_p = 0):\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.drop1 = nn.Dropout(mlp_p)\n",
    "        self.fc2 = nn.Linear(hidden_features,out_features)\n",
    "        self.drop2 = nn.Dropout(mlp_p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 mlp_ratio=4,\n",
    "                 embed_dim=768,\n",
    "                 num_heads=12,\n",
    "                 attn_p = 0,\n",
    "                 mlp_p = 0,\n",
    "                 proj_p = 0):\n",
    "        \n",
    "        super(Block, self).__init__()\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = SelfAttentionEncoder(embed_dim=embed_dim, num_heads=num_heads, attn_p=attn_p, proj_p=proj_p)\n",
    "        self.mlp = MLP(embed_dim, int(embed_dim*mlp_ratio), embed_dim, mlp_p)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \n",
    "        x = x + self.attention(self.layer_norm1(x), attention_mask)\n",
    "        x = x + self.mlp(self.layer_norm2(x))\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c673e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 50265])\n"
     ]
    }
   ],
   "source": [
    "class RoBERTa(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 max_seq_len=512,\n",
    "                 vocab_size=tokenizer.vocab_size,\n",
    "                 embed_dim=768,\n",
    "                 num_heads=12,\n",
    "                 depth=12,\n",
    "                 mlp_ratio=4,\n",
    "                 attn_p=0,\n",
    "                 pos_p=0,\n",
    "                 proj_p=0,\n",
    "                 mlp_p=0):\n",
    "        super(RoBERTa, self).__init__()\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
    "        self.pos_embeddings = nn.Embedding(max_seq_len+1, embed_dim)\n",
    "        self.pos_drop = nn.Dropout(pos_p)\n",
    "        \n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    mlp_ratio=mlp_ratio, \n",
    "                    embed_dim=embed_dim, \n",
    "                    num_heads=num_heads, \n",
    "                    attn_p=attn_p, \n",
    "                    proj_p=proj_p,\n",
    "                    mlp_p=mlp_p)\n",
    "                    \n",
    "                    for _ in range(depth)\n",
    "                \n",
    "            ]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        device = x.device\n",
    "        \n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        if seq_len > self.max_seq_len:\n",
    "            x = x[:,-self.max_seq_len:]\n",
    "            \n",
    "        avail_idx = torch.arange(0, seq_len+1, dtype=torch.long, device=device)\n",
    "        #print(avail_idx) #tensor([0, 1, 2, 3, 4, 5]) where the cls token + rest of 5 index for tensor\n",
    "        tok_embed = self.embeddings(x)\n",
    "        #print(tok_embed.shape) # torch.Size([2, 5, 768])\n",
    "        #print(self.cls_token.shape) # torch.Size([1,1,768])\n",
    "        \n",
    "        ##################################################\n",
    "        ## Appending cls token in front of the tensor\n",
    "        cls_token = self.cls_token.expand(batch_size, -1,-1)\n",
    "        #print(cls_token.shape) # torch.Size([2, 1, 768])\n",
    "        ##################################################\n",
    "        \n",
    "        ## Concatenate the cls token infront of the token embedding\n",
    "        tok_embed = torch.cat((cls_token,tok_embed),dim=1) \n",
    "        #print(tok_embed.shape) # torch.Size([2, 6, 768])\n",
    "        \n",
    "        ##################################################\n",
    "        ## Apply the positional embedding to the available indexes\n",
    "        pos_embed = self.pos_embeddings(avail_idx)\n",
    "        \n",
    "        ## Now add token_embedding with the Positional embedding for position information purpose\n",
    "        x = tok_embed + pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        ## apply the logic on the blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        ## Slicing off the first cls token we dont need right now \n",
    "        cls_token_final = x[:,0]\n",
    "        \n",
    "        ## Slicing off the remaining tokens\n",
    "        x = x[:,1:]\n",
    "        \n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "## Test Case\n",
    "        \n",
    "rand_x = torch.randint(0,10, (2,5))\n",
    "padding = torch.tensor([[True,True,True,True,True],[True,True,True,True,False]])\n",
    "roberta = RoBERTa()\n",
    "\n",
    "out = roberta(rand_x,padding)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c392208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "RoBERTa(\n",
      "  (embeddings): Embedding(50265, 384)\n",
      "  (pos_embeddings): Embedding(101, 384)\n",
      "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0-3): 4 x Block(\n",
      "      (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): SelfAttentionEncoder(\n",
      "        (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (k_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0, inplace=False)\n",
      "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (drop2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): Linear(in_features=384, out_features=50265, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### DEFINE TRAINING PARAMETERS ###\n",
    "iterations = 15000\n",
    "max_len = 100\n",
    "evaluate_interval = 100\n",
    "embedding_dim = 384\n",
    "depth = 4\n",
    "num_heads = 4\n",
    "lr = 0.0005\n",
    "batch_size = 64\n",
    "\n",
    "DEVICE = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "### DEFINE MODEL AND OPTIMIZER ###\n",
    "model = RoBERTa(max_seq_len=max_len, \n",
    "                embed_dim=embedding_dim, \n",
    "                depth=depth, \n",
    "                num_heads=num_heads, \n",
    "                attn_p=0.1, \n",
    "                mlp_p=0.1, \n",
    "                proj_p=0.1, \n",
    "                pos_p=0.1)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "print(model)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "### DEFINE LOSS FUNCTION ###\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "### Build DataLoader ###\n",
    "dataset = MaskedLMLoader(tokenized_text, max_seq_len=max_len)\n",
    "trainset, testset = torch.utils.data.random_split(dataset, [int(0.95*len(dataset)),int(len(dataset) - int(0.95*len(dataset)))])\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "### Define Scheduler ###\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer, \n",
    "                                            num_warmup_steps=1500, \n",
    "                                            num_training_steps=iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72959b94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
